from src.textfab.units import *
from textfab.fabric import Fabric


class TestUnits:
    def test_punck_remove(self):
        unit = remove_punct()
        assert unit.process("test, case") == "test case"

    def test_enter_to_space(self):
        unit = swap_enter_to_space()
        assert unit.process("test\ncase") == "test case"

    def test_swap_enter_to_sapce(self):
        unit = collapse_spaces()
        assert unit.process("test    case") == "test case"

    def test_remove_latin(self):
        unit = remove_latin()
        assert (
            unit.process("–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å categorical –∏–ª–∏ continious")
            == "–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å  –∏–ª–∏ "
        )

    def test_remove_non_cyrillic(self):
        unit = remove_non_cyrillic()
        assert unit.process("–¢–µst #2 –¥–ª—è renive –Ω–µ cyrillic") == "–¢–µ  –¥–ª—è  –Ω–µ "

    def test_remove_custom_regex(self):
        unit = remove_custom_regex({"regex": "[0-9]+"})
        assert (
            unit.process("This is 1st text since 1343 . It contains 323 signs")
            == "This is st text since  . It contains  signs"
        )

    def test_lemmatize_by_mystem(self):
        unit = lemmatize_by_mystem()
        assert (
            unit.process("–ö—Ä–∞—Å–∏–≤–∞—è –º–∞–º–∞ –∫—Ä–∞—Å–∏–≤–æ –º—ã–ª–∞ —Ä–∞–º—É")
            == "–∫—Ä–∞—Å–∏–≤—ã–π –º–∞–º–∞ –∫—Ä–∞—Å–∏–≤–æ –º—ã—Ç—å —Ä–∞–º–∞\n"
        )

    def test_remove_emoji(self):
        unit = remove_emoji()
        assert unit.process("–ü—Ä–∏–≤–µ—ÇüòÄ") == "–ü—Ä–∏–≤–µ—Ç"

    def test_remove_accents(self):
        unit = remove_accents()
        assert unit.process("–ú–æ–¥–µ—Ä–Ω–∏–∑–∞ÃÅ—Ü–∏—è") == "–ú–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏—è"

    def test_emoji_tokenizer(self):
        unit = tokenize_with_emoji()
        assert unit.process("–ü—Ä–∏–≤–µ—Ç!üòÄ –ö–∞–∫ –¥–µ–ª–∞?") == [
            "–ü—Ä–∏–≤–µ—Ç",
            "!",
            "üòÄ",
            "–ö–∞–∫",
            "–¥–µ–ª–∞",
            "?",
        ]

    def test_tokenize_with_nltkr(self):
        unit = tokenize_with_nltk()
        assert unit.process("–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?") == ["–ü—Ä–∏–≤–µ—Ç", "!", "–ö–∞–∫", "–¥–µ–ª–∞", "?"]

    def test_segment_by_sentences(self):
        unit = segment_by_sentences({"language": "russian"})
        text = """<pa>–ê–Ω–∞–ª–∏–∑ —á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏ –ö–ì–ú –ø—Ä–æ–≤–æ–¥–∏–ª–∏ —á–∞—à–µ—á–Ω—ã–º –º–µ—Ç–æ–¥–æ–º –ö–æ—Ö–∞, –Ω–∞ —Å—Ä–µ–¥–µ –°–ú–ú (—Å—Ä–µ–¥–∞ –¥–ª—è –º–æ—Ä—Å–∫–∏—Ö –º–∏–∫—Ä–æ–æ—Ä–≥–∞–Ω–∏–∑–º–æ–≤) . 
        –ë–ì–ö–ü –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–ª–µ–∫—Ç–∏–≤–Ω–æ–π —Å—Ä–µ–¥—ã –≠–Ω–¥–æ. –û–ø—Ä–µ–¥–µ–ª—è–ª–∏ –∫–∞—Ç–∞–ª–∞–∑–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ, –æ–∫—Å–∏–¥–∞–∑–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ, –≥—Ä–∞–º–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –±–∞–∫—Ç–µ—Ä–∏–∏ .  
        –ù–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–∫—Ç–µ—Ä–∏–π –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∏–∑–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≥—Ä—É–ø–ø ‚Äì –ù–û, –§–î, –î–¢ –æ—Ü–µ–Ω–∏–≤–∞–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–æ–¥–∞ –ø—Ä–µ–¥–µ–ª—å–Ω—ã—Ö —Ä–∞–∑–≤–µ–¥–µ–Ω–∏–π —Å 
        –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç–ª–µ–∫—Ç–∏–≤–Ω–æ–π —Å—Ä–µ–¥—ã –ú–ö–î, —Å–æ–¥–µ—Ä–∂–∞—â–µ–π –≤ –∫–∞—á–µ—Å—Ç–≤–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —É–≥–ª–µ—Ä–æ–¥–∞ –¥–∏–∑–µ–ª—å–Ω–æ–µ —Ç–æ–ø–ª–∏–≤–æ, –Ω–µ—Ñ—Ç—å, —Ñ–µ–Ω–æ–ª –≤ –∫–æ–Ω–µ—á–Ω–æ–π –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ 0.1% ."""
        assert unit.process(text) == [
            "<pa>–ê–Ω–∞–ª–∏–∑ —á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏ –ö–ì–ú –ø—Ä–æ–≤–æ–¥–∏–ª–∏ —á–∞—à–µ—á–Ω—ã–º –º–µ—Ç–æ–¥–æ–º –ö–æ—Ö–∞, –Ω–∞ —Å—Ä–µ–¥–µ –°–ú–ú (—Å—Ä–µ–¥–∞ –¥–ª—è –º–æ—Ä—Å–∫–∏—Ö –º–∏–∫—Ä–æ–æ—Ä–≥–∞–Ω–∏–∑–º–æ–≤) .",
            "–ë–ì–ö–ü –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–ª–µ–∫—Ç–∏–≤–Ω–æ–π —Å—Ä–µ–¥—ã –≠–Ω–¥–æ.",
            "–û–ø—Ä–µ–¥–µ–ª—è–ª–∏ –∫–∞—Ç–∞–ª–∞–∑–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ, –æ–∫—Å–∏–¥–∞–∑–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ, –≥—Ä–∞–º–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –±–∞–∫—Ç–µ—Ä–∏–∏ .",
            "–ù–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–∫—Ç–µ—Ä–∏–π –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∏–∑–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≥—Ä—É–ø–ø ‚Äì –ù–û, –§–î, –î–¢ –æ—Ü–µ–Ω–∏–≤–∞–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–æ–¥–∞ –ø—Ä–µ–¥–µ–ª—å–Ω—ã—Ö —Ä–∞–∑–≤–µ–¥–µ–Ω–∏–π —Å \n        –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç–ª–µ–∫—Ç–∏–≤–Ω–æ–π —Å—Ä–µ–¥—ã –ú–ö–î, —Å–æ–¥–µ—Ä–∂–∞—â–µ–π –≤ –∫–∞—á–µ—Å—Ç–≤–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —É–≥–ª–µ—Ä–æ–¥–∞ –¥–∏–∑–µ–ª—å–Ω–æ–µ —Ç–æ–ø–ª–∏–≤–æ, –Ω–µ—Ñ—Ç—å, —Ñ–µ–Ω–æ–ª –≤ –∫–æ–Ω–µ—á–Ω–æ–π –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ 0.1% .",
        ]

    def test_remove_links(self):
        unit = remove_links()
        text = "–ø—Ä–∏–≤–µ—Ç. –ø–æ–¥–ø–∏—à–∏—Å—å –Ω–∞ https://github.com/Astromis/textfab/issues/10 –∏ –µ—â–µ —Å—é–¥–∞ https://www.google.com/search?q=dsf&sca_esv=595131700&source=hp&ei=S1KUZabvG4_BwPAP0dOHkAo&iflsig=AO6bgOgAAAAAZZRgWymwyPoYTEAdamM1XaAgdt6BMs60&ved=0ahUKEwjmwN-lp7-DAxWPIBAIHdHpAaIQ4dUDCAk&uact=5&oq=dsf&gs_lp=Egdnd3Mtd2l6IgNkc2YyCBAAGIAEGLEDMgUQLhiABDIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAESLwCUABYGHAAeACQAQCYAaMBoAG2AqoBAzIuMbgBA8gBAPgBAcICERAuGIAEGLEDGIMBGMcBGNEDwgILEAAYgAQYsQMYgwHCAggQLhiABBixA8ICBBAAGAPCAhcQLhiABBiKBRixAxiDARjHARivARiOBcICDhAuGIAEGLEDGMcBGNED&sclient=gws-wiz"
        assert unit.process(text) == "–ø—Ä–∏–≤–µ—Ç. –ø–æ–¥–ø–∏—à–∏—Å—å –Ω–∞  –∏ –µ—â–µ —Å—é–¥–∞ "

    def test_remove_mobile_phone_numbers(self):
        unit = remove_mobile_phone_numbers()
        text = """89103123167|+7-910-221-22-22|+7(910)-221-22-22"""
        assert all([unit.process(x).strip() == "" for x in text.split("|")])

    def test_detokenize_with_space(self):
        unit = detokenize_with_space()
        text = ["part", "one", "part", "two"]
        assert "part one part two" == unit.process(text)

    def test_apply_butter_finger(self):
        unit = apply_butter_finger({"prob_token_pass": 0.5, "seed": 43})
        text = ["—ç—Ç–æ", "—Ç–µ—Å—Ç–æ–≤—ã–π", "–Ω–∞–±–æ—Ä", "–¥–ª—è", "–∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π"]
        assert ["—ç—Ç–æ", "—Ç–µ—Å—Ç–æ–≤–∞–π", "–Ω–∞–±–æ–Ω", "–¥—Ç—è", "–∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π"] == unit.process(text)

    def test_apply_changing_token_char_case(self):
        unit = apply_changing_token_char_case({"try_numbers": 1, "seed": 42})
        text = ["—ç—Ç–æ", "—Ç–µ—Å—Ç–æ–≤—ã–π", "–Ω–∞–±–æ—Ä", "–¥–ª—è", "–∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π"]
        assert ["–≠—Ç–æ", "—Ç–µ–°—Ç–û–≤—ã–ô", "–Ω–∞–±–æ—Ä", "–¥–ª—è", "–∞—É–ì–ú–µ–Ω—Ç–∞—Ü–∏–π"] == unit.process(text)

    def test_apply_apply_random_token_deletion(self):
        unit = apply_random_token_deletion({"try_numbers": 2, "seed": 43})
        text = ["—ç—Ç–æ", "—Ç–µ—Å—Ç–æ–≤—ã–π", "–Ω–∞–±–æ—Ä", "–¥–ª—è", "–∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π"]
        assert ["—Ç–µ—Å—Ç–æ–≤—ã–π", "–Ω–∞–±–æ—Ä", "–¥–ª—è"] == unit.process(text)

    def test_apply_random_token_swap(self):
        unit = apply_random_token_swap({"try_numbers": 1, "seed": 44})
        text = ["—ç—Ç–æ", "—Ç–µ—Å—Ç–æ–≤—ã–π", "–Ω–∞–±–æ—Ä", "–¥–ª—è", "–∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π"]
        assert ["—ç—Ç–æ", "—Ç–µ—Å—Ç–æ–≤—ã–π", "–Ω–∞–±–æ—Ä", "–∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π", "–¥–ª—è"] == unit.process(text)


def test_conv():
    config = [
        "swap_enter_to_space",
        "remove_punct",
        "collapse_spaces",
    ]
    conv = Fabric(config)
    assert conv(["This text, is\n\n for test"]) == ["This text is for test"]


def test_parralel_conv():
    config = [
        "swap_enter_to_space",
        "remove_punct",
        "collapse_spaces",
    ]
    conv = Fabric(config,)
    assert conv(
        [
            "This text, is\n\n for test",
            "This another text, is\n\n for test",
            "This yet another text, is\n\n for test",
        ],
        pool_size=5,
    ) == [
        "This text is for test",
        "This another text is for test",
        "This yet another text is for test",
    ]


def test_param_units():
    config = ["remove_punct", {"remove_custom_regex": {"regex": "a"}}]
    conv = Fabric(config)
    assert conv(["This is a test string."]) == ["This is  test string"]
